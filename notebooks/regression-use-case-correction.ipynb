{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valentin-Laurent/MAPIE-DataCraft/blob/main/notebooks/regression-use-case-correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# A Concrete Use Case of Regression with MAPIE\n",
        "\n",
        "## Context\n",
        "\n",
        "California is divided into districts, each requiring a license to operate as a real estate professional.* Every 10 years, a population census is conducted, and the public data is a goldmine for real estate developers in the state of California.\n",
        "\n",
        "This data contains the following information:\n",
        "\n",
        "| Variable   | Description |\n",
        "|------------|------------|\n",
        "| `MedHouseVal` | Median house value in a given district (in hundreds of thousands of dollars). |\n",
        "| `MedInc`     | Median household income in the district (in tens of thousands of dollars). |\n",
        "| `HouseAge`   | Median house age in the district (in years). |\n",
        "| `AveRooms`   | Average number of rooms per dwelling. |\n",
        "| `AveBedrms`  | Average number of bedrooms per dwelling. |\n",
        "| `Population` | Total population of the district. |\n",
        "| `AveOccup`   | Average number of people per dwelling. |\n",
        "| `Latitude`   | Latitude of the district center. |\n",
        "| `Longitude`  | Longitude of the district center. |\n",
        "\n",
        "Bill Smith, a real-estate developer, seeks your help. Indeed, as with previous censuses, some districts, known as \"sneaky\" districts, do not wish to publish data related to house values in their area. This information is crucial to estimate whether it is worthwhile to pursue a license in a given district.\n",
        "\n",
        "The datasets available to you are:\n",
        "  - `X` and `y`: data from districts that have published all their figures (`y` contains the `MedHouseVal` information, and `X` the other variables)\n",
        "  - `X_sneaky`: data from sneaky districts (without the `MedHouseVal` information)\n",
        "\n",
        "During the previous census, Mr. Smith had to mobilize his teams for over a week to estimate this missing data, district by district. This time, he wants to automate the process.\n",
        "\n",
        "After several workshops with him, you defined a deliverable in the form of a map, highlighting in green the sneaky districts whose median house value does not exceed $150,000. Bill seems rather risk-averse and prefers to obtain a reduced number of districts that we are sure about.\n",
        "\n",
        "*Note: this use case is fictional, so we have taken liberties with the actual laws in California!\n",
        "\n",
        "## Action Plan\n",
        "1. Quick exploratory data analysis\n",
        "2. Training a GradientBoostingRegressor model, as it is powerful and supports quantile loss (we will see the benefit of this loss later)\n",
        "3. Conformalizing the model using the ConformalizedQuantileRegressor method\n",
        "4. Predicting confidence intervals on the X_sneaky dataset for the `MedHouseVal` variable\n",
        "5. Displaying on a map the sneaky districts that meet Bill's criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "source": [
        "# Imports and Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b04428",
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /content/MAPIE-DataCraft\n",
        "!git clone https://github.com/Valentin-Laurent/MAPIE-DataCraft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eda6502",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mapie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/MAPIE-DataCraft/notebooks/use_case_files')\n",
        "\n",
        "import folium\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from scipy.stats import uniform, randint\n",
        "from mapie.metrics.regression import regression_coverage_score, regression_mean_width_score\n",
        "from mapie.regression import ConformalizedQuantileRegressor\n",
        "from mapie.utils import train_conformalize_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3",
      "metadata": {
        "id": "3"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv(\"/content/MAPIE-DataCraft/notebooks/use_case_files/X.csv\")\n",
        "y = pd.read_csv(\"/content/MAPIE-DataCraft/notebooks/use_case_files/y.csv\").squeeze(\"columns\")\n",
        "X_sneaky = pd.read_csv(\"/content/MAPIE-DataCraft/notebooks/use_case_files/X_sneaky.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "# Quick Exploratory Data Analysis (Provided)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "## Null Values and Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "Let's check if the X dataset has any null values or duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "print(X.info(), \"\\n\")\n",
        "print(\"Duplicates:\", X.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "source": [
        "## Variable Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=y)\n",
        ")\n",
        "fig.update_layout(\n",
        "    height=350,\n",
        "    width=600,\n",
        "    xaxis_title=\"Median Price (100,000$)\",\n",
        "    title_text=\"Target\",\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": [],
      "source": [
        "rows, cols = 3, 3\n",
        "fig = make_subplots(rows=rows, cols=cols, subplot_titles=X.columns)\n",
        "\n",
        "for i, col in enumerate(X.columns):\n",
        "    row = i // cols + 1\n",
        "    col_num = i % cols + 1\n",
        "    fig.add_trace(go.Histogram(y=X[col], name=col), row=row, col=col_num)\n",
        "\n",
        "fig.update_layout(\n",
        "    height=700,\n",
        "    width=1200,\n",
        "    title_text=\"Histograms\",\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "## Correlations Between Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": [
        "corr_matrix = X.corr()\n",
        "\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=np.abs(corr_matrix.values),\n",
        "    x=list(corr_matrix.columns),\n",
        "    y=list(corr_matrix.index),\n",
        "    annotation_text=corr_matrix.round(2).values,\n",
        "    showscale=True\n",
        ")\n",
        "fig.update_layout(\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "# Training a Predictive Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "source": [
        "## Splitting Training and Conformity Data\n",
        "\n",
        "For \"split\" methods included in MAPIE (`SplitConformalRegressor` and `ConformalizedQuantileRegressor`), the training and conformity datasets are used as follows:\n",
        "\n",
        "![title](https://github.com/Valentin-Laurent/MAPIE-DataCraft/blob/main/notebooks/use_case_files/data-split.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "**Exercise 1**: In the previous tutorial, we used the `ConformalizedQuantileRegressor` by letting MAPIE handle the model training. This time, we will use the `prefit=True` mode with a `GradientBoostingRegressor` trained by us. We need a training set, a test set, and, to use MAPIE, a conformity set.\n",
        "\n",
        "Create a training set (`X_train`, `y_train`), a conformity set (`X_conformalize`, `y_conformalize`) and a test set (`X_test`, `y_test`) using `train_conformalize_test_split`, and so that:\n",
        "  - 80% of the data is used for the training;\n",
        "  - The conformity set and the test set are of equal sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "X_train, X_conformalize, X_test, y_train, y_conformalize, y_test = train_conformalize_test_split(X, y, train_size=0.8, conformalize_size=0.1, test_size=0.1, random_state=RANDOM_STATE)  # correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "source": [
        "We will use a `GradientBoostingRegressor`, which is quite robust to correlated variables and asymmetric distributions.\n",
        "\n",
        "**Exercise 2**:\n",
        "  - Perform a hyperparameter search on a sklearn `GradientBoostingRegressor` model using the `RandomizedSearchCV` method (which allows cross-validation).\n",
        "  - Do not exceed 50 trainings (this number depends on the parameter grid searched and the number of folds).\n",
        "  - Use `loss=\"absolute_error\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "outputs": [],
      "source": [
        "param_distributions = {\n",
        "    \"n_estimators\": randint(50, 500),\n",
        "    \"learning_rate\": uniform(0.01, 0.3),\n",
        "    \"max_depth\": randint(2, 10),\n",
        "    \"subsample\": uniform(0.5, 0.5),\n",
        "    \"min_samples_split\": randint(2, 20),\n",
        "    \"min_samples_leaf\": randint(1, 20),\n",
        "    \"max_features\": uniform(0.5, 0.5)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "outputs": [],
      "source": [
        "random_search = RandomizedSearchCV(  # correction\n",
        "  GradientBoostingRegressor(loss=\"absolute_error\", random_state=RANDOM_STATE),  # correction\n",
        "  param_distributions,  # correction\n",
        "  n_iter=15,  # correction\n",
        "  cv=3,  # correction\n",
        "  scoring=\"neg_mean_absolute_error\",  # correction\n",
        "  n_jobs=-1,  # correction\n",
        "  verbose=1,  # correction\n",
        "  random_state=RANDOM_STATE,  # correction\n",
        ")  # correction\n",
        "random_search.fit(X_train, y_train)  # correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "source": [
        "**Exercise 3**: For the most performant model:\n",
        "- Display its hyperparameters\n",
        "- Its average MAE on the cross-validation sets\n",
        "- Its MAE on the test set\n",
        "- Interpretation of these values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23",
      "metadata": {
        "id": "23"
      },
      "outputs": [],
      "source": [
        "regressor = random_search.best_estimator_  # correction\n",
        "print(\"Best parameters:\", random_search.best_params_, \"\\n\")  # correction\n",
        "print(\"MAE on training set\", -random_search.best_score_)  # correction\n",
        "print(\"On the training set, our model is wrong on average by\", round(-random_search.best_score_ * 100), \"k$.\")  # correction\n",
        "y_pred = regressor.predict(X_test)  # correction\n",
        "print(\"On the test set, our model is wrong on average by\", round(mean_absolute_error(y_test, y_pred)*100), \"k$.\")  # correction\n",
        "# Our model does not seem to have overfit as the MAE on the training and test sets are similar  # correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test,\n",
        "    y=y_pred,\n",
        "    mode=\"markers\",\n",
        "    name=\"Predictions\",\n",
        "    marker=dict(color=\"blue\", opacity=0.6)\n",
        "))\n",
        "\n",
        "min_val, max_val = np.min(y_test), np.max(y_test)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val, max_val],\n",
        "    y=[min_val, max_val],\n",
        "    mode=\"lines\",\n",
        "    name=\"Line y = x\",\n",
        "    line=dict(color=\"red\", dash=\"dash\")\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Comparison between y_test and y_pred\",\n",
        "    xaxis_title=\"y_test (100,000$)\",\n",
        "    yaxis_title=\"y_pred (100,000$)\",\n",
        "    showlegend=True,\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25",
      "metadata": {
        "id": "25"
      },
      "source": [
        "## Calculating Intervals with MAPIE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "### Explanation of the `ConformalizedQuantileRegressor`\n",
        "This method requires not 1 but 3 models: a usual model trained to predict the target variable (our model trained above), and two models trained to predict the 5% and 95% quantiles of the target variable, respectively. The idea is that the target variable falls 90% of the time between the predictions of our two quantile models at 5% and 95%.\n",
        "\n",
        "Finally, the conformalization phase allows \"calibrating\" these models to provide the theoretical guarantees specific to conformal predictions.\n",
        "\n",
        "![title](https://github.com/Valentin-Laurent/MAPIE-DataCraft/blob/main/notebooks/use_case_files/quantiles.png?raw=1)\n",
        "\n",
        "We must therefore train two additional models corresponding to the upper and lower quantiles before using MAPIE."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "source": [
        "### Using the `ConformalizedQuantileRegressor`\n",
        "\n",
        "**Exercise 4**: Train the two quantile models with the same hyperparameters as the main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "outputs": [],
      "source": [
        "confidence_level = 0.90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "outputs": [],
      "source": [
        "regressor_lower_quantile = GradientBoostingRegressor(  # correction\n",
        "  **random_search.best_params_,   # correction\n",
        "  loss=\"quantile\",  # correction\n",
        "  alpha=(1-confidence_level)/2,  # correction\n",
        "  random_state=RANDOM_STATE  # correction\n",
        ").fit(X_train, y_train)  # correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "outputs": [],
      "source": [
        "regressor_upper_quantile = GradientBoostingRegressor(  # correction\n",
        "  **random_search.best_params_,  # correction\n",
        "  loss=\"quantile\",  # correction\n",
        "  alpha=(1+confidence_level)/2,  # correction\n",
        "  random_state=RANDOM_STATE  # correction\n",
        ").fit(X_train, y_train)  # correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "source": [
        "**Exercise 5**: Out of curiosity, let's look at the empirical coverage rates on the test set that these models provide *before* conformalization with MAPIE. Create `y_pred_lower_quantile_before_mapie` and `y_pred_upper_quantile_before_mapie`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "outputs": [],
      "source": [
        "y_pred_lower_quantile_before_mapie = regressor_lower_quantile.predict(X_test)  # correction\n",
        "y_pred_upper_quantile_before_mapie = regressor_upper_quantile.predict(X_test)  # correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "y_pred_combined = np.vstack((y_pred_lower_quantile_before_mapie, y_pred_upper_quantile_before_mapie)).T\n",
        "print(f\"Empirical coverage rate before MAPIE: {regression_coverage_score(y_test, y_pred_combined)[0]:.3f}\")\n",
        "y_pred_combined_expanded = y_pred_combined.reshape(y_pred_combined.shape[0], y_pred_combined.shape[1], 1)\n",
        "print(f\"Average interval width before MAPIE: {round(regression_mean_width_score(y_pred_combined_expanded)[0], 2)} (100k$)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame((y_pred_upper_quantile_before_mapie - y_pred_lower_quantile_before_mapie), columns=[\"Interval Width\"])\n",
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Histogram(\n",
        "        x=df[\"Interval Width\"]\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title_text=\"Distribution of Interval Widths (100,000$)\",\n",
        "    xaxis_title=\"Interval Size (100,000$)\",\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "source": [
        "What to think of this empirical coverage rate? Let's now use MAPIE to obtain more robust coverage guarantees:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "source": [
        "**Exercise 6**: Use the `ConformalizedQuantileRegressor` in `prefit` mode to predict `y_preds_test` and `y_pred_intervals_test` on `X_test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "mapie_regressor = ConformalizedQuantileRegressor(  # correction\n",
        "  estimator=[regressor_lower_quantile, regressor_upper_quantile, regressor],  # correction\n",
        "  confidence_level=confidence_level,  # correction\n",
        "  prefit=True  # correction\n",
        ")  # correction\n",
        "mapie_regressor.conformalize(X_conformalize, y_conformalize)  # correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "outputs": [],
      "source": [
        "y_preds_test, y_pred_intervals_test = mapie_regressor.predict_interval(  # correction\n",
        "  X_test  # correction\n",
        ")  # correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "source": [
        "Let's check our empirical coverage rate on the test set and calculate the average size of our intervals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "outputs": [],
      "source": [
        "y_pred_lower_quantile = y_pred_intervals_test[:, 0, 0]\n",
        "y_pred_upper_quantile = y_pred_intervals_test[:, 1, 0]\n",
        "\n",
        "y_pred_combined = np.vstack((y_pred_lower_quantile, y_pred_upper_quantile)).T\n",
        "print(f\"Empirical coverage rate: {regression_coverage_score(y_test, y_pred_combined)[0]:.3f}\")\n",
        "y_pred_combined_expanded = y_pred_combined.reshape(y_pred_combined.shape[0], y_pred_combined.shape[1], 1)\n",
        "print(f\"Average interval width: {round(regression_mean_width_score(y_pred_combined_expanded)[0], 2)} (100 000$)\")\n",
        "\n",
        "intervals_before = (y_pred_upper_quantile_before_mapie - y_pred_lower_quantile_before_mapie)\n",
        "intervals_after = (y_pred_upper_quantile - y_pred_lower_quantile)\n",
        "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.15,\n",
        "                    subplot_titles=[\"Before MAPIE\", \"After MAPIE\"])\n",
        "fig.add_trace(go.Histogram(x=intervals_before, name=\"Before MAPIE\", nbinsx=20), row=1, col=1)\n",
        "fig.add_trace(go.Histogram(x=intervals_after, name=\"After MAPIE\", nbinsx=20), row=2, col=1)\n",
        "fig.update_layout(\n",
        "    title_text=\"Distribution of Interval Widths (100,000$)\",\n",
        "    showlegend=False,\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "source": [
        "Is the coverage rate satisfactory? Is it at the expense of another indicator of interest in our problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test[0::30],\n",
        "    y=y_pred[0::30],\n",
        "    error_y=dict(\n",
        "        type=\"data\",\n",
        "        array=intervals_after,\n",
        "    ),\n",
        "    mode=\"markers\",\n",
        "    name=\"Predictions\",\n",
        "    marker=dict(color=\"blue\", opacity=0.6)\n",
        "))\n",
        "\n",
        "min_val, max_val = np.min(y_test), np.max(y_test)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val, max_val],\n",
        "    y=[min_val, max_val],\n",
        "    mode=\"lines\",\n",
        "    name=\"Line y = x\",\n",
        "    line=dict(color=\"red\", dash=\"dash\")\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Comparison between y_test and y_pred with error bars (on a subset for better readability)\",\n",
        "    xaxis_title=\"y_test (100,000$)\",\n",
        "    yaxis_title=\"y_pred (100,000$)\",\n",
        "    showlegend=True,\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "source": [
        "## Analysis of Green Districts on the Test Set\n",
        "\n",
        "Remember: the agreed deliverable with Bill is a map highlighting in green the sneaky districts whose median house value does not exceed $150,000. And Bill prefers to obtain a reduced number of districts that we are sure about.\n",
        "\n",
        "We can, for example, define \"green districts\" as follows: those whose entire interval predicted by MAPIE is below the $150k threshold:\n",
        "\n",
        "![title](https://github.com/Valentin-Laurent/MAPIE-DataCraft/blob/main/notebooks/use_case_files/green-districts.png?raw=1)\n",
        "\n",
        "**Exercise 7** - difficult ;)\n",
        "- We chose a coverage level of 90%. What statistical guarantee can we communicate to Bill about the green districts?\n",
        "- We can, for example, look at the coverage rate on green districts only. Create a Numpy boolean array named `green_districts_mask` from `y_pred_upper_quantile`. An element of this array is `True` if the district meets the condition given above, and `False` otherwise. Then, calculate the coverage rate on green districts only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": [
        "# The coverage rate applies to all our intervals. That is, on average across ALL districts, our coverage rate will be statistically above 90%.  # correction\n",
        "# The coverage rate on green districts can therefore be different. We talk about \"conditional coverage\". We can estimate an empirical rate by looking at its value on the test set:  # correction\n",
        "green_districts_mask = y_pred_upper_quantile <= 1.5  # correction\n",
        "y_pred_upper_quantile_green = y_pred_upper_quantile[green_districts_mask]  # correction\n",
        "y_pred_lower_quantile_green = y_pred_lower_quantile[green_districts_mask]  # correction\n",
        "y_pred_upper_quantile_red = y_pred_upper_quantile[~green_districts_mask]  # correction\n",
        "y_pred_lower_quantile_red = y_pred_lower_quantile[~green_districts_mask]  # correction\n",
        "y_test_green = y_test[green_districts_mask]  # correction\n",
        "y_test_red = y_test[~green_districts_mask]  # correction\n",
        "y_pred_green = np.vstack((y_pred_lower_quantile_green, y_pred_upper_quantile_green)).T   # correction\n",
        "y_pred_red = np.vstack((y_pred_lower_quantile_red, y_pred_upper_quantile_red)).T  # correction\n",
        "fig = go.Figure(go.Bar(  # correction\n",
        "    x=[\"Coverage Rate of Non-Green Districts\", \"Coverage Rate of Green Districts\"],  # correction\n",
        "    y=[  # correction\n",
        "        regression_coverage_score(y_test_red, y_pred_red)[0],  # correction\n",
        "        regression_coverage_score(y_test_green, y_pred_green)[0]  # correction\n",
        "    ],  # correction\n",
        "    text=[f\"{len(y_test_red)} districts\", f\"{len(y_test_green)} districts\"],  # correction\n",
        "    textposition=\"auto\",  # correction\n",
        "    marker=dict(color=[\"red\", \"green\"])  # correction\n",
        "))  # correction\n",
        "fig.update_layout(  # correction\n",
        "    title=\"Empirical Coverage Rates on the Test Set\",  # correction\n",
        "    #xaxis_title=\"Series\",  # correction\n",
        "    #yaxis_title=\"Number of observations\",  # correction\n",
        "    font=dict(family=\"Computer Modern\", size=18, color=\"#7f7f7f\")  # correction\n",
        ")  # correction\n",
        "fig.show()  # correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45",
      "metadata": {
        "id": "45"
      },
      "source": [
        "Our coverage rate seems better when it comes to green districts! This is reassuring information, even though the only statistical guarantee we can provide to Bill is on **all** sneaky districts. #  correction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46",
      "metadata": {
        "id": "46"
      },
      "source": [
        "# Producing the Deliverable in the Form of a Map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47",
      "metadata": {
        "id": "47"
      },
      "source": [
        "**Exercise 8**:\n",
        "- Use MAPIE to calculate `y_preds_sneaky` and `y_pred_intervals_sneaky`\n",
        "- Create a Numpy array `green_sneaky_districts` containing the list of sneaky districts we want to display (`True`) or not (`False`) from `y_pred_intervals_sneaky`\n",
        "- Create a DataFrame `map_data` containing the columns `Latitude`, `Longitude`, and `Green district`\n",
        "- Use the folium code to display the map requested by Bill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48",
      "metadata": {
        "id": "48"
      },
      "outputs": [],
      "source": [
        "y_preds_sneaky, y_pred_intervals_sneaky = mapie_regressor.predict_interval(X_sneaky)  # correction\n",
        "green_sneaky_districts = y_pred_intervals_sneaky[:, 1, 0] <= 1.5  # correction\n",
        "map_data = X_sneaky[[\"Latitude\", \"Longitude\"]].copy()  # correction\n",
        "map_data[\"Green district\"] = green_sneaky_districts  # correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49",
      "metadata": {
        "id": "49"
      },
      "outputs": [],
      "source": [
        "california_map = folium.Map(location=[36.7783, -119.4179], zoom_start=6, min_zoom=5)\n",
        "\n",
        "for _, row in map_data.iterrows():\n",
        "    if row[\"Green district\"]:\n",
        "        folium.CircleMarker(\n",
        "            location=[row['Latitude'], row['Longitude']],\n",
        "            radius=3,\n",
        "            color=\"green\",\n",
        "            fill=True,\n",
        "            fill_opacity=0.7,\n",
        "        ).add_to(california_map)\n",
        "\n",
        "n_sneaky_district_total = X_sneaky.shape[0]\n",
        "n_green_district = map_data[\"Green district\"].sum()\n",
        "print(\"Total number of sneaky districts\", n_sneaky_district_total)\n",
        "print(\"Number of green districts\", n_green_district)\n",
        "print(\"Number of districts not displayed on the map\", n_sneaky_district_total - n_green_district)\n",
        "\n",
        "california_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50",
      "metadata": {
        "id": "50"
      },
      "source": [
        "# Feedback from Bill\n",
        "\n",
        "Bill is satisfied with this first deliverable and understands the statistical significance of your results. He wants to continue working with you to refine the work provided.\n",
        "\n",
        "In particular, he would like to know the sneaky districts that are uncertain but promising. That is, those where:\n",
        " - the house value predicted by the model is below $150,000\n",
        " - but the upper bound predicted by MAPIE exceeds this threshold\n",
        "\n",
        "You have educated Bill on the limitations of AI, and he understands that he will have to make a case-by-case decision with his teams for these districts.\n",
        "\n",
        "Let's refine the deliverable by displaying these uncertain districts in orange.\n",
        "\n",
        "![title](https://github.com/Valentin-Laurent/MAPIE-DataCraft/blob/main/notebooks/use_case_files/orange-districts.png?raw=1)\n",
        "\n",
        "**Exercise 9**:\n",
        "- Create a Numpy array `orange_sneaky_districts` containing the list of uncertain districts we want to display (`True`) or not (`False`) from `y_preds_sneaky` and `y_pred_intervals_sneaky`.\n",
        "- Add the `Orange district` column to the `map_data` DataFrame\n",
        "- Display the new map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51",
      "metadata": {
        "id": "51"
      },
      "outputs": [],
      "source": [
        "orange_sneaky_districts = (y_preds_sneaky <= 1.5) & (y_pred_intervals_sneaky[:, 0, 0] <= 1.5) & (y_pred_intervals_sneaky[:, 1, 0] > 1.5)  # correction\n",
        "map_data[\"Orange district\"] = orange_sneaky_districts  # correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52",
      "metadata": {
        "id": "52"
      },
      "outputs": [],
      "source": [
        "california_map = folium.Map(location=[36.7783, -119.4179], zoom_start=6, min_zoom=5)\n",
        "\n",
        "for _, row in map_data.iterrows():\n",
        "    if row[\"Green district\"]:\n",
        "        folium.CircleMarker(\n",
        "            location=[row['Latitude'], row['Longitude']],\n",
        "            radius=3,\n",
        "            color=\"green\",\n",
        "            fill=True,\n",
        "            fill_opacity=0.7,\n",
        "        ).add_to(california_map)\n",
        "\n",
        "for _, row in map_data.iterrows():\n",
        "    if row[\"Orange district\"]:\n",
        "        folium.CircleMarker(\n",
        "            location=[row['Latitude'], row['Longitude']],\n",
        "            radius=1,\n",
        "            color=\"orange\",\n",
        "            fill=True,\n",
        "            fill_opacity=0.7,\n",
        "        ).add_to(california_map)\n",
        "\n",
        "n_orange_district = map_data[\"Orange district\"].sum()\n",
        "print(\"Total number of sneaky districts\", n_sneaky_district_total)\n",
        "print(\"Number of green districts\", n_green_district)\n",
        "print(\"Number of orange districts\", n_orange_district)\n",
        "print(\"Number of districts not displayed on the map\", n_sneaky_district_total - n_green_district - n_orange_district)\n",
        "\n",
        "california_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53",
      "metadata": {
        "id": "53"
      },
      "source": [
        "Congratulations! You are now an expert in conformal predictions ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54",
      "metadata": {
        "id": "54"
      },
      "source": [
        "# To Go Further"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55",
      "metadata": {
        "id": "55"
      },
      "source": [
        "Some open questions to further reflect on this use case:\n",
        "1. Imagine Bill is not risk-averse but rather a daredevil: What confidence level choices would you make? What choice between green and orange districts?\n",
        "2. Bill informs us that the total number of rooms is an indicator of interest in his profitability calculations. We can look at the conditional coverage rate based on the total number of rooms (for example, by dividing this variable into categories: 0-10, 10-20, 30-40, 40-50). Are the coverage rates corresponding to these different categories homogeneous? If not, what remediation method could we use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "se4RrvsAknmC",
      "metadata": {
        "id": "se4RrvsAknmC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
